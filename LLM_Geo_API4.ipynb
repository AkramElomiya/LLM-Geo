{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf676855-2221-47c7-8dff-b185e03c3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05156c09-3ec8-47d7-a629-f9374440658a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install pyvis\n",
    "# ! pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e9bd-675a-4644-abfd-d642183da809",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392efdfe-e33d-43fb-b8ea-6b6fb444ad9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyvis.network import Network\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e43a82-d40f-4442-93fb-525486017ca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Solution class\n",
    "Please run the following cell to define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "236eff8d-4f24-4e90-af5f-4847bf2e7112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import LLM_Geo_Constants as constants\n",
    "import helper\n",
    "# import LLM_Geo_kernel.Solution as Solution\n",
    "\n",
    "from LLM_Geo_kernel import Solution\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e84cf-00f5-40ef-a7ff-5ae186a4e164",
   "metadata": {},
   "source": [
    "# Demonstration 1: Resident living with hazardous wastes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a545a6b-1456-40b8-a913-b4dfd305c071",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input task and data desciption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf073e47-9e79-41d3-bc84-500590ad2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DATA_LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba05f98-30b2-46d0-9eb2-624f5dbc2754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt to get solution graph:\n",
      "\n",
      "Your role: A professional Geo-information scientist and developer good at Python. \n",
      "Task: Generate a graph (data structure) only, whose nodes are (1) a series of consecutive steps and (2) data to solve this question:  \n",
      " 1) Find out the Autism service providers' addresses from their website. Their address usually listed in the 'about' or 'contact' page, you can crawl the links (in the homepage) contain 'about' or 'contact'.\n",
      "2) The address shoul contain street, city, state, and zipcode. E.g., 1234 NW Bobcat Lane, St. Robert, MO 65584. A provider may have multiple service address. If you cannot find the address, simply return nothing, DO NOT make up fake addresses. \n",
      "3) If cannot find the street address, the city, state, or zipcode are also need to return. In summary, find the address information as much as possible.\n",
      "4) You need to use ChatGPT to extract address, design prompt and then attach the webpage text ChatGPT, ask it to extract address. Use this pre-written function to get response from ChatGPT: helper.get_LLM_reply(prompt=your_prompt_with_webpage_text). Use this statement go extract content returned response: response['choices'][0]['message']['content']. \n",
      "5) Save the extracted addresses as \"Address\" column, together with the given 'Provider' and 'Web Site' columns. If there are multile addresses for a provider, each address is a row in the CSV file.\n",
      " \n",
      "Data locations (each data is a node): 1. Autism service provider webpage file location: https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv. The 'Web Site' column is the URL, the 'Provider' column is the provider name. \n",
      "Your reply needs to meet these requirements: \n",
      " 1. Think step by step.\n",
      "2. steps and data (both input and output) form a graph stored in NetworkX. Diconnected components are NOT allowed.\n",
      "3. Each step is a data process operation: the input can be data paths or variables, and the output can be data paths or variables.\n",
      "4. There are two types of nodes: a) operation node, and b) data node (both input and output data). These nodes are also input nodes for the next operation node.\n",
      "5. The input of each operation is the output of the previous operations, except the those need to load data from a path or need to collect data.\n",
      "6. You need to carefully named the output data node.\n",
      "7. The data and operation form a graph.\n",
      "8. The first operations are data loading or collection, and the output of the last operation is the final answer to the task.Operation nodes need to connect via output data nodes, DO NOT connect the operation node directly.\n",
      "9. The node attributes include: 1) node_type (data or operation), 2) data_path (data node only, set to \"\" if not given ), and description. E.g., {‘name’: “County boundary”, “data_type”: “data”, “data_path”: “D:\\Test\\county.shp”,  “description”: “County boundary for the study area”}.\n",
      "10. The connection between a node and an operation node is an edge.\n",
      "11. Add all nodes and edges, including node attributes to a NetworkX instance, DO NOT change the attribute names.\n",
      "12. DO NOT generate code to implement the steps.\n",
      "13. Join the attribute to the vector layer via a common attribute if necessery.\n",
      "14. Put your reply into a Python code block, NO explanation or conversation outside the code block(enclosed by ```python and ```).\n",
      "15. Note that GraphML writer does not support class dict or list as data values.\n",
      "16. You need spatial data (e.g., vector or raster) to make a map.\n",
      "17. Do not put the GraphML writing process as a step in the graph.\n",
      "18. Save the network into GraphML format, save it at: E:\\Research\\LLM-Geo\\Address_extraction\\Address_extraction.graphml \n",
      " \n",
      "Reply example: \n",
      "```python\n",
      "import networkx as nx\n",
      "G = nx.DiGraph()\n",
      "# Add nodes and edges for the graph\n",
      "# 1 Load hazardous waste site shapefile\n",
      "G.add_node(\"haz_waste_shp_url\", node_type=\"data\", path=\"https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/Hazardous_Waste_Sites.zip\", description=\"Hazardous waste facility shapefile URL\")\n",
      "G.add_node(\"load_haz_waste_shp\", node_type=\"operation\", description=\"Load hazardous waste facility shapefile\")\n",
      "G.add_edge(\"haz_waste_shp_url\", \"load_haz_waste_shp\")\n",
      "G.add_node(\"haz_waste_gdf\", node_type=\"data\", description=\"Hazardous waste facility GeoDataFrame\")\n",
      "G.add_edge(\"load_haz_waste_shp\", \"haz_waste_gdf\")\n",
      "...\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 1: population living near hazardous waster\n",
    "'''\n",
    "TASK = r\"\"\"1) Find out the total population that lives within a tract that contain hazardous waste facilities. The study area is North Carolina, US.\n",
    "2) Generate a map to show the spatial distribution of population at the tract level and highlight the borders of tracts that have hazardous waste facilities.\n",
    "\"\"\"\n",
    "\n",
    "DATA_LOCATIONS = [\"NC hazardous waste facility ESRI shape file location: https://github.com/gladcolor/LLM- Geo/raw/master/overlay_analysis/Hazardous_Waste_Sites.zip.\",\n",
    "                  \"NC tract boundary shapefile location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/tract_shp_37.zip. The tract id column is 'Tract'.\",\n",
    "                  \"NC tract population CSV file location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/NC_tract_population.csv. The population is stored in 'TotalPopulation' column. The tract ID column is 'GEOID'.\"\n",
    "                 ]\n",
    "'''\n",
    "\n",
    "# Case 2: mobility data retrieval and visulization\n",
    "\"\"\"\n",
    "TASK = r'''\n",
    "1) Show the monthly change rates of each administrative regions in a France map. Each month is a sub-map in a map matrix. The base of the change rate is January 2020. \n",
    "2) Draw a line chart to show the monthly change rate trends of all administrative regeions.\n",
    "\n",
    "'''\n",
    "\n",
    "DATA_LOCATIONS = [\"ESRI shapefile for France administrative regions:\" + \\\n",
    "                  \"https://github.com/gladcolor/LLM-Geo/raw/master/REST_API/France.zip.\" + \\\n",
    "                  \"The 'GID_1' column is the administrative region code, 'NAME_1' column is the administrative region name.\",\n",
    "                  \"REST API url with parameters for mobility data access:\" + \\\n",
    "                  \"http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=world_first_level_admin&begin=01/01/2020&end=12/31/2020.\" + \\\n",
    "                  \"The response is in CSV format. There are three columns in the response: \" + \\\n",
    "                  \"place,date (format:2020-01-07), and intra_movement. 'place' column is the administractive region code, France administrative regions start with 'FRA'.\",\n",
    "                 ]\n",
    "\n",
    "# Bug: \n",
    "# base_month =  \"2020-01\" # wrong: pd.to_datetime(\"2020-01\")\n",
    "# print(region_monthly[region_monthly['month_year'] == base_month])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Case 3: mobility data retrieval and visulization\n",
    "TASK = r\"\"\"1) Find out the Autism service providers' addresses from their website. Their address usually listed in the 'about' or 'contact' page, you can crawl the links (in the homepage) contain 'about' or 'contact'.\n",
    "2) The address shoul contain street, city, state, and zipcode. E.g., 1234 NW Bobcat Lane, St. Robert, MO 65584. A provider may have multiple service address. If you cannot find the address, simply return nothing, DO NOT make up fake addresses. \n",
    "3) If cannot find the street address, the city, state, or zipcode are also need to return. In summary, find the address information as much as possible.\n",
    "4) You need to use ChatGPT to extract address, design prompt and then attach the webpage text ChatGPT, ask it to extract address. Use this pre-written function to get response from ChatGPT: helper.get_LLM_reply(prompt=your_prompt_with_webpage_text). Use this statement go extract content returned response: response['choices'][0]['message']['content']. \n",
    "5) Save the extracted addresses as \"Address\" column, together with the given 'Provider' and 'Web Site' columns. If there are multile addresses for a provider, each address is a row in the CSV file.\n",
    "\"\"\"\n",
    "\n",
    "DATA_LOCATIONS = [\"Autism service provider webpage file location: https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv. The 'Web Site' column is the URL, the 'Provider' column is the provider name.\",                  \n",
    "                 ]\n",
    "\n",
    "\n",
    "# TASK = r'''1)Retrieve the data from the REST API and plot the intra_movement column of the returned data as line chart to show the temporal trend of all states. \n",
    "# 2) plot the temporal trend of the movement for each state. Each state figure will be sub figure in the plot. The plot has 5 columns. In addition, please add a weekly smoothed line to each sub plot, and change the line color to orange.\n",
    "# 3) Using the REST API with date range from 01/01/2020 to 12/31/2020 to analyze the movement reduction rate for each state during two periods: the first period is 01/01/2020-02/29/2020, second period is 03/01/2020 to 04/30/2020. Please find out the reduction rate for each state during the two periods, and create a table to report the result with two columns: state name, reduction rate, sorted by reduction rate.\n",
    "# '''\n",
    "# '''\n",
    "# DATA_LOCATIONS = [\"REST API url with parameters for data access: http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=us_state&begin=01/01/2020&end=12/31/2020; The response is in CSV format. There are three columns in the response: place,date,intra_movement; place refers to the state name.\"\n",
    "#                  ]\n",
    "# '''\n",
    "# 3) Show the administrative region name in the map and chart.\n",
    "# \n",
    "# task_name ='Resident_at_risk_counting'\n",
    "# task_name ='France_mobility_changes_2020'  \n",
    "task_name ='Address_extraction'  \n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), task_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# create graph\n",
    "# model=r\"gpt-3.5-turbo\"\n",
    "model=r\"gpt-4\"\n",
    "model=r\"gpt-4\"\n",
    "solution = Solution(\n",
    "                    task=TASK,\n",
    "                    task_name=task_name,\n",
    "                    save_dir=save_dir,\n",
    "                    data_locations=DATA_LOCATIONS,\n",
    "                    model=model,\n",
    "                    )\n",
    "print(\"Prompt to get solution graph:\\n\")\n",
    "print(solution.graph_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9eeba-7eeb-4f4b-b6ae-7e0cd94c547b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3009da7e-afe7-48da-a0c0-62d5a10026fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get graph code from GPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "213b17ca-9e3e-4c23-a852-c4a7edc83448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "\n",
      "Code to generate solution graph: \n",
      "\n",
      "import networkx as nx\n",
      "\n",
      "G = nx.DiGraph()\n",
      "\n",
      "# 1. Load provider CSV file\n",
      "G.add_node(\"provider_csv_url\", node_type=\"data\", path=\"https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv\", description=\"Autism service provider CSV file URL\")\n",
      "G.add_node(\"load_provider_csv\", node_type=\"operation\", description=\"Load Autism service provider CSV\")\n",
      "G.add_edge(\"provider_csv_url\", \"load_provider_csv\")\n",
      "\n",
      "G.add_node(\"provider_df\", node_type=\"data\", description=\"Autism service provider DataFrame\")\n",
      "G.add_edge(\"load_provider_csv\", \"provider_df\")\n",
      "\n",
      "# 2. Crawl contact or about website pages for addresses\n",
      "G.add_node(\"crawl_website_for_address\", node_type=\"operation\", description=\"Crawl website for addresses\")\n",
      "G.add_edge(\"provider_df\", \"crawl_website_for_address\")\n",
      "\n",
      "G.add_node(\"webpage_texts\", node_type=\"data\", description=\"Texts obtained from crawled webpages\")\n",
      "G.add_edge(\"crawl_website_for_address\", \"webpage_texts\")\n",
      "\n",
      "# 3. Extract addresses using ChatGPT\n",
      "G.add_node(\"extract_addresses\", node_type=\"operation\", description=\"Extract addresses using ChatGPT\")\n",
      "G.add_edge(\"webpage_texts\", \"extract_addresses\")\n",
      "\n",
      "G.add_node(\"addresses_list\", node_type=\"data\", description=\"List of extracted addresses\")\n",
      "G.add_edge(\"extract_addresses\", \"addresses_list\")\n",
      "\n",
      "# 4. Combine addresses with the original information\n",
      "G.add_node(\"combine_addresses_and_providers\", node_type=\"operation\", description=\"Combine extracted addresses with the original provider information\")\n",
      "G.add_edge(\"addresses_list\", \"combine_addresses_and_providers\")\n",
      "G.add_edge(\"provider_df\", \"combine_addresses_and_providers\")\n",
      "\n",
      "G.add_node(\"final_provider_df\", node_type=\"data\", description=\"Updated provider DataFrame with addresses\")\n",
      "G.add_edge(\"combine_addresses_and_providers\", \"final_provider_df\")\n",
      "\n",
      "# Save the graph to GraphML format\n",
      "nx.write_graphml(G, 'E:\\\\Research\\\\LLM-Geo\\\\Address_extraction\\\\Address_extraction.graphml')\n"
     ]
    }
   ],
   "source": [
    "response_for_graph = solution.get_LLM_response_for_graph() \n",
    "solution.graph_response = response_for_graph\n",
    "solution.save_solution()\n",
    "print()\n",
    "print(\"Code to generate solution graph: \\n\")\n",
    "print(solution.code_for_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112017eb-8bcb-4d44-88d5-7099f22bf107",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execute code to generate the solution graphto generate the solution graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1695d40-b164-4d8b-8381-957ce260a5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Research\\LLM-Geo\\Address_extraction.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"E:\\Research\\LLM-Geo\\Address_extraction.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x23dcb93bee0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(solution.code_for_graph)\n",
    "solution_graph = solution.load_graph_file()\n",
    "\n",
    "# Show the graph\n",
    "G = nx.read_graphml(solution.graph_file)  \n",
    "nt = helper.show_graph(G)\n",
    "html_name = os.path.join(os.getcwd(), solution.task_name + '.html')  \n",
    "# HTML file should in the same directory. See:\n",
    "# https://stackoverflow.com/questions/65564916/error-displaying-pyvis-html-inside-jupyter-lab-cell\n",
    "nt.show(name=html_name)\n",
    "# html_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f8311-1874-4b23-957c-793cfd74a9cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate prompts and code for operations (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "italic-appearance",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with open(r'F:\\Research\\LLM-Geo\\Resident_at_risk_counting\\Resident_at_risk_counting.pkl', 'rb') as f:\n",
    "#     solution = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4459452-8156-476d-8d20-1fccf3fdaa48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 4, load_provider_csv\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "2 / 4, crawl_website_for_address\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "3 / 4, extract_addresses\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "4 / 4, combine_addresses_and_providers\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n"
     ]
    }
   ],
   "source": [
    "operations = solution.get_LLM_responses_for_operations()\n",
    "solution.save_solution()\n",
    "\n",
    "# all_operation_code_str = '\\n'.join([operation['operation_code'] for operation in operations])\n",
    "# print(\"All operation code: \\n\")\n",
    "# print(all_operation_code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cc36daf-008e-4159-bc07-23552a9249c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "def load_provider_csv(provider_csv_url=\"https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv\"):\n",
      "    \"\"\"\n",
      "    Load Autism service provider CSV.\n",
      "    \n",
      "    Arguments:\n",
      "    - provider_csv_url: The URL of the CSV file containing Autism service provider data (default: given URL)\n",
      "\n",
      "    Returns:\n",
      "    - provider_df: A pandas DataFrame containing the Autism service provider data\n",
      "    \"\"\"\n",
      "    provider_df = pd.read_csv(provider_csv_url)\n",
      "    return provider_df\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import pandas as pd\n",
      "from tqdm import tqdm\n",
      "\n",
      "def crawl_website_for_address(provider_df):\n",
      "    \"\"\"\n",
      "    Crawl website for addresses\n",
      "\n",
      "    Arguments:\n",
      "    - provider_df: A pandas DataFrame containing the Autism service provider data\n",
      "\n",
      "    Returns:\n",
      "    - webpage_texts: A dictionary where the key is the provider name and the value is the text content of each 'about' or 'contact' page for further address extraction\n",
      "    \"\"\"\n",
      "    def get_links(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "        about_or_contact_links = []\n",
      "\n",
      "        for link in soup.find_all(\"a\", href=True):\n",
      "            link_text = link.get(\"href\").lower()\n",
      "            if \"about\" in link_text or \"contact\" in link_text:\n",
      "                about_or_contact_links.append(link[\"href\"])\n",
      "\n",
      "        return about_or_contact_links\n",
      "\n",
      "    def get_text(link, base_url):\n",
      "        response = requests.get(f\"{base_url.rstrip('/')}/{link.lstrip('/')}\")\n",
      "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "        return soup.get_text()\n",
      "\n",
      "    webpage_texts = {}\n",
      "\n",
      "    for idx, provider_row in tqdm(provider_df.iterrows(), total=len(provider_df)):\n",
      "        provider = provider_row['Provider']\n",
      "        base_url = provider_row['Web Site']\n",
      "        webpage_texts[provider] = []\n",
      "\n",
      "        try:\n",
      "            about_or_contact_links = get_links(base_url)\n",
      "            for link in about_or_contact_links:\n",
      "                try:\n",
      "                    text = get_text(link, base_url)\n",
      "                    webpage_texts[provider].append(text)\n",
      "                except requests.exceptions.RequestException:\n",
      "                    continue\n",
      "        except requests.exceptions.RequestException:\n",
      "            continue\n",
      "\n",
      "    return webpage_texts\n",
      "import pandas as pd\n",
      "from helper import get_LLM_reply\n",
      "\n",
      "def extract_addresses(webpage_texts):\n",
      "    \"\"\"\n",
      "    Extract addresses using ChatGPT\n",
      "\n",
      "    Arguments:\n",
      "    - webpage_texts: A dictionary where the key is the provider name and the value is the text content of each 'about' or 'contact' page\n",
      "\n",
      "    Returns:\n",
      "    - addresses_list: A list of dictionaries containing extracted addresses, each dictionary has two keys - 'Provider' and 'Address'\n",
      "    \"\"\"\n",
      "    addresses_list = []\n",
      "\n",
      "    for provider, texts in webpage_texts.items():\n",
      "        for text in texts:\n",
      "            prompt = f\"Extract the address of the Autism service provider from the given text: \\\"{text}\\\"\"\n",
      "            try:\n",
      "                response = get_LLM_reply(prompt=prompt)\n",
      "                extracted_address = response['choices'][0]['message']['content']\n",
      "\n",
      "                if extracted_address:\n",
      "                    addresses_list.append({'Provider': provider, 'Address': extracted_address})\n",
      "                \n",
      "            except Exception as e:\n",
      "                continue\n",
      "\n",
      "    return addresses_list\n",
      "import pandas as pd\n",
      "\n",
      "def combine_addresses_and_providers(provider_df, addresses_list):\n",
      "    \"\"\"\n",
      "    Combine extracted addresses with the original provider information\n",
      "\n",
      "    Arguments:\n",
      "    - provider_df: A pandas DataFrame containing the Autism service provider data\n",
      "    - addresses_list: A list of dictionaries containing extracted addresses, each dictionary has two keys - 'Provider' and 'Address'\n",
      "\n",
      "    Returns:\n",
      "    - final_provider_df: A pandas DataFrame containing the original provider information merged with the extracted addresses as a new 'Address' column\n",
      "    \"\"\"\n",
      "    address_df = pd.DataFrame(addresses_list)\n",
      "    final_provider_df = pd.merge(provider_df, address_df, on=\"Provider\", how=\"left\")\n",
      "    return final_provider_df\n"
     ]
    }
   ],
   "source": [
    "all_operation_code_str = '\\n'.join([operation['operation_code'] for operation in operations])\n",
    "print(all_operation_code_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21f735-29f4-4934-8a7e-00616447cd40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate prompts and code for assembly program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34ac055d-11b8-4b3e-890c-83501611355f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "def main():\n",
      "    # Step 1: Load Autism service provider CSV\n",
      "    provider_df = load_provider_csv()\n",
      "\n",
      "    # Step 2: Crawl website for addresses\n",
      "    webpage_texts = crawl_website_for_address(provider_df)\n",
      "\n",
      "    # Step 3: Extract addresses using ChatGPT\n",
      "    addresses_list = extract_addresses(webpage_texts)\n",
      "\n",
      "    # Step 4: Combine extracted addresses with the original provider information\n",
      "    final_provider_df = combine_addresses_and_providers(provider_df, addresses_list)\n",
      "\n",
      "    # Step 5: Save the final_provider_df as a CSV file\n",
      "    final_provider_df.to_csv(\"final_provider_addresses.csv\", index=False)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "Assembly code: \n",
      "\n",
      "def main():\n",
      "    # Step 1: Load Autism service provider CSV\n",
      "    provider_df = load_provider_csv()\n",
      "\n",
      "    # Step 2: Crawl website for addresses\n",
      "    webpage_texts = crawl_website_for_address(provider_df)\n",
      "\n",
      "    # Step 3: Extract addresses using ChatGPT\n",
      "    addresses_list = extract_addresses(webpage_texts)\n",
      "\n",
      "    # Step 4: Combine extracted addresses with the original provider information\n",
      "    final_provider_df = combine_addresses_and_providers(provider_df, addresses_list)\n",
      "\n",
      "    # Step 5: Save the final_provider_df as a CSV file\n",
      "    final_provider_df.to_csv(\"final_provider_addresses.csv\", index=False)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "assembly_LLM_response = solution.get_LLM_assembly_response()\n",
    "# solution.assembly_LLM_response = assembly_LLM_response\n",
    "solution.save_solution()\n",
    "\n",
    "print(\"Assembly code: \\n\")\n",
    "print(solution.code_for_assembly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca67352-6508-4fe6-be4b-1f4649224148",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execute assembly code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "583d7c6a-8d5a-4bca-b22e-fcb3f3d5c201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def main():\n",
      "    providers_df = load_provider_csv()\n",
      "    websites_and_providers = extract_website_and_provider(providers_df)\n",
      "    webpage_contents = crawl_websites(websites_and_providers)\n",
      "    addresses = extract_addresses(webpage_contents)\n",
      "    output_file = 'Provider_Addresses.csv'\n",
      "    save_csv(addresses, websites_and_providers, output_file)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "all_code = all_operation_code_str + '\\n' + solution.code_for_assembly\n",
    "print(solution.code_for_assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90365390-4b0a-4164-8b25-5179d13da5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(all_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0eb3c-7bb1-4f61-988b-281cbfe9ee6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████▊                                                        | 399/1381 [11:54<21:28,  1.31s/it]"
     ]
    }
   ],
   "source": [
    "providers_df = load_provider_csv()\n",
    "websites_and_providers = extract_website_and_provider(providers_df)\n",
    "webpage_contents = crawl_websites(websites_and_providers)\n",
    "addresses = extract_addresses(webpage_contents)\n",
    "output_file = 'Provider_Addresses.csv'\n",
    "save_csv(addresses, websites_and_providers, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c61f4-fa4d-4456-8fc5-1759c6198ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5aac09-fb66-4d6a-978d-3be5c25d3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Case 2: mobility \n",
    "# import geopandas as gpd\n",
    "\n",
    "# def load_france_shp(france_shp_url=\"https://github.com/gladcolor/LLM-Geo/raw/master/REST_API/France.zip\"):\n",
    "#     \"\"\"\n",
    "#     Description: Load France administrative regions shapefile\n",
    "\n",
    "#     Args:\n",
    "#     france_shp_url (str): URL of the zipped shapefile\n",
    "\n",
    "#     Returns:\n",
    "#     france_gdf (GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "#     \"\"\"\n",
    "#     france_gdf = gpd.read_file(france_shp_url)\n",
    "#     return france_gdf\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import requests\n",
    "# from io import StringIO\n",
    "\n",
    "# def load_mobility_data(mobility_api_url=\"http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=world_first_level_admin&begin=01/01/2020&end=12/31/2020\"):\n",
    "#     \"\"\"\n",
    "#     Load COVID-19 mobility data\n",
    "\n",
    "#     mobility_api_url: REST API url for COVID-19 mobility data\n",
    "#     returns: mobility_data (dataframe): Mobility data with place, date and intra_movement values\n",
    "#     \"\"\"\n",
    "#     response = requests.get(mobility_api_url)\n",
    "#     data = StringIO(response.text)\n",
    "#     mobility_data = pd.read_csv(data)\n",
    "\n",
    "#     return mobility_data\n",
    "# def join_data(france_gdf=gpd.GeoDataFrame(), mobility_data=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Join shapefile and mobility data\n",
    "\n",
    "#     Args:\n",
    "#     france_gdf (GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "#     mobility_data (pd.DataFrame): Mobility data with place, date, and intra_movement values\n",
    "\n",
    "#     Returns:\n",
    "#     joined_data (GeoDataFrame): GeoDataFrame containing the joined shapefile and mobility data\n",
    "#     \"\"\"\n",
    "#     # Convert GID_1 column to string type without leading zeros\n",
    "#     france_gdf[\"GID_1\"] = france_gdf[\"GID_1\"].astype(str).str.lstrip(\"0\")\n",
    "\n",
    "#     # Convert place column to string type without leading zeros\n",
    "#     mobility_data[\"place\"] = mobility_data[\"place\"].astype(str).str.lstrip(\"0\")\n",
    "\n",
    "#     # Merge mobility data with shapefile based on GID_1 and place columns\n",
    "#     joined_data = france_gdf.merge(mobility_data, left_on=\"GID_1\", right_on=\"place\")\n",
    "\n",
    "#     # Remove duplicates\n",
    "#     joined_data.drop_duplicates(subset=[\"GID_1\", \"date\"], inplace=True)\n",
    "\n",
    "#     return joined_data\n",
    "# def calc_monthly_changes(joined_data=gpd.GeoDataFrame()):\n",
    "#     \"\"\"\n",
    "#     Create monthly change rates\n",
    "    \n",
    "#     Args:\n",
    "#     joined_data (gpd.GeoDataFrame): GeoDataFrame containing the joined shapefile and mobility data\n",
    "    \n",
    "#     Returns:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "#     \"\"\"\n",
    "#     # Parse dates and create a month-year column\n",
    "#     joined_data['date'] = pd.to_datetime(joined_data['date'])\n",
    "#     joined_data['month_year'] = joined_data['date'].dt.to_period('M')\n",
    "\n",
    "#     # Calculate the sum of intra_movement for each region and month-year\n",
    "#     region_monthly = joined_data.groupby(['NAME_1', 'month_year'])['intra_movement'].sum().reset_index()\n",
    "\n",
    "#     # Calculate the changes of each month based on January 2020\n",
    "#     base_month =  \"2020-01\" # pd.to_datetime(\"2020-01\")\n",
    "#     print(region_monthly[region_monthly['month_year'] == \"2020-01\"])\n",
    "#     baseline = region_monthly[region_monthly['month_year'] == base_month].set_index('NAME_1')['intra_movement']\n",
    "#     print(baseline)\n",
    "#     region_monthly['change_rate'] = region_monthly.apply(lambda row: (row['intra_movement'] - baseline[row['NAME_1']]) / baseline[row['NAME_1']], axis=1)\n",
    "\n",
    "#     # Pivot month_year as columns to create monthly_changes DataFrame\n",
    "#     monthly_changes = region_monthly.pivot(index='NAME_1', columns='month_year', values='change_rate').reset_index()\n",
    "\n",
    "#     return monthly_changes\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def matrix_map(monthly_changes=pd.DataFrame(), france_gdf=gpd.GeoDataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Create matrix of France maps\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "#     france_gdf (gpd.GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "\n",
    "#     Returns:\n",
    "#     france_map_fig (Figure): Figure containing the matrix of France maps with monthly change rates\n",
    "#     \"\"\"\n",
    "#     # Merge monthly_changes with france_gdf\n",
    "#     france_gdf = france_gdf.merge(monthly_changes, on=\"NAME_1\")\n",
    "    \n",
    "#     # Set up plot\n",
    "#     plt.style.use('seaborn-darkgrid')\n",
    "#     france_map_fig, axs = plt.subplots(3, 4, figsize=(24, 18), sharex='col', sharey='row')\n",
    "#     france_map_fig.suptitle('Monthly change rates of each administrative region in France')\n",
    "\n",
    "#     # Iterate through columns for each month\n",
    "#     for idx, month_year in enumerate(monthly_changes.columns[1:]):\n",
    "#         row, col = divmod(idx, 4)\n",
    "        \n",
    "#         # Create subplot\n",
    "#         france_gdf.plot(column=month_year, ax=axs[row, col], legend=True, cmap='coolwarm', legend_kwds={'shrink': 0.8})\n",
    "#         axs[row, col].set_title(month_year.strftime(\"%B %Y\"))\n",
    "#         axs[row, col].set_xticks([])\n",
    "#         axs[row, col].set_yticks([])\n",
    "#         axs[row, col].set_xlabel(\"\")\n",
    "#         axs[row, col].set_ylabel(\"\")\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "#     return france_map_fig\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def chart_matrix(monthly_changes=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Create chart matrix of each administrative region's line chart\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "\n",
    "#     Returns:\n",
    "#     line_chart_matrix_fig (matplotlib.figure.Figure): Figure object containing the line chart matrix\n",
    "#     \"\"\"\n",
    "#     # Set default seaborn style\n",
    "#     sns.set()\n",
    "\n",
    "#     # Prepare monthly_changes data\n",
    "#     monthly_changes_tidy = pd.melt(monthly_changes, \n",
    "#                                    id_vars=[\"NAME_1\"], \n",
    "#                                    var_name=\"month_year\", \n",
    "#                                    value_name=\"change_rate\")\n",
    "#     monthly_changes_tidy[\"month_year\"] = monthly_changes_tidy[\"month_year\"].astype(str)\n",
    "\n",
    "#     # Calculate the number of rows and columns for the line chart matrix\n",
    "#     n_regions = len(monthly_changes[\"NAME_1\"].unique())\n",
    "#     n_cols = min(4, n_regions)\n",
    "#     n_rows = (n_regions // n_cols) + (1 if (n_regions % n_cols) else 0)\n",
    "\n",
    "#     # Create the line chart matrix\n",
    "#     line_chart_matrix_fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5), sharex=True, sharey=True)\n",
    "\n",
    "#     for idx, (name, group) in enumerate(monthly_changes_tidy.groupby([\"NAME_1\"])):\n",
    "#         row, col = divmod(idx, n_cols)\n",
    "\n",
    "#         ax = axes[row][col]\n",
    "#         sns.lineplot(x=\"month_year\", y=\"change_rate\", data=group, ax=ax)\n",
    "#         ax.set_title(name)\n",
    "#         ax.set_xticklabels(labels=group[\"month_year\"].unique(), rotation=45)\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for idx in range(n_regions, n_rows * n_cols):\n",
    "#         row, col = divmod(idx, n_cols)\n",
    "#         axes[row][col].remove()\n",
    "\n",
    "#     # Adjust layout\n",
    "#     line_chart_matrix_fig.tight_layout()\n",
    "\n",
    "#     return line_chart_matrix_fig\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def overall_trends_line_chart(monthly_changes=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Draw the line chart for all administrative regions\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "\n",
    "#     Returns:\n",
    "#     trends_line_chart_fig (matplotlib.figure.Figure): Figure containing the line chart for all administrative regions\n",
    "#     \"\"\"\n",
    "#     # Transpose monthly_changes DataFrame\n",
    "#     transposed_changes = monthly_changes.set_index(\"NAME_1\").transpose()\n",
    "\n",
    "#     # Create line chart\n",
    "#     trends_line_chart_fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     transposed_changes.plot(ax=ax)\n",
    "#     plt.xlabel(\"Month\")\n",
    "#     plt.ylabel(\"Change Rate\")\n",
    "#     plt.title(\"Monthly Trends of Change Rates for All Administrative Regions\")\n",
    "#     plt.legend(title=\"Regions\", labels=transposed_changes.columns, bbox_to_anchor=(1.17, 1))\n",
    "#     plt.xticks(range(len(transposed_changes.index)), transposed_changes.index, rotation=90)\n",
    "\n",
    "#     return trends_line_chart_fig\n",
    "\n",
    "# # Example usage\n",
    "# # france_gdf = load_france_shp()\n",
    "# # mobility_data = load_mobility_data()\n",
    "# # joined_data = join_data(france_gdf, mobility_data)\n",
    "# # monthly_changes = calc_monthly_changes(joined_data)\n",
    "# # trends_line_chart_fig = overall_trends_line_chart(monthly_changes)\n",
    "# # plt.show()\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from io import StringIO\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    " \n",
    "# # Load data\n",
    "# monthly_changes = calc_monthly_changes(joined_data)\n",
    "\n",
    "# # Create and save monthly change maps\n",
    "# france_map_fig = matrix_map(monthly_changes, france_gdf)\n",
    "# france_map_fig.savefig('france_monthly_change_maps.png')\n",
    "\n",
    "# # Create and save chart matrix with line charts of each administrative region\n",
    "# line_chart_matrix_fig = chart_matrix(monthly_changes)\n",
    "# line_chart_matrix_fig.savefig('chart_matrix_line_charts.png')\n",
    "\n",
    "# # Create and save overall trends line chart\n",
    "# trends_line_chart_fig = overall_trends_line_chart(monthly_changes)\n",
    "# trends_line_chart_fig.savefig('overall_trends_line_chart.png')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111a633-11ad-4501-83b0-658a81353c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data#.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "street_mapping_env",
   "language": "python",
   "name": "street_mapping_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
