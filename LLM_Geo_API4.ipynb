{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf676855-2221-47c7-8dff-b185e03c3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05156c09-3ec8-47d7-a629-f9374440658a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install pyvis\n",
    "# ! pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e9bd-675a-4644-abfd-d642183da809",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "392efdfe-e33d-43fb-b8ea-6b6fb444ad9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyvis.network import Network\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e43a82-d40f-4442-93fb-525486017ca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Solution class\n",
    "Please run the following cell to define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "236eff8d-4f24-4e90-af5f-4847bf2e7112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import LLM_Geo_Constants as constants\n",
    "import helper\n",
    "# import LLM_Geo_kernel.Solution as Solution\n",
    "\n",
    "from LLM_Geo_kernel import Solution\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e84cf-00f5-40ef-a7ff-5ae186a4e164",
   "metadata": {},
   "source": [
    "# Demonstration Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a545a6b-1456-40b8-a913-b4dfd305c071",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input task and data desciption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf073e47-9e79-41d3-bc84-500590ad2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DATA_LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ba05f98-30b2-46d0-9eb2-624f5dbc2754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt to get solution graph:\n",
      "\n",
      "Your role: A professional Geo-information scientist and developer good at Python. \n",
      "Task: Generate a graph (data structure) only, whose nodes are (1) a series of consecutive steps and (2) data to solve this question:  \n",
      " 1) Find out the Autism service providers' addresses from their website. The address usually listed in the homepage, 'about' or 'contact' page. The latter two pages usually can be found from links in the homepage: theire URLs usually contain 'about' or 'contact'.\n",
      "2) The address shoul contain street, city, state, and zipcode. E.g., 1234 NW Bobcat Lane, St. Robert, MO 65584. A provider may have multiple service address. If you cannot find the address, simply return an empty text, DO NOT make up fake addresses. \n",
      "3) If cannot find the street address, the city, state, or zipcode are also need to return. In summary, find the address information as much as possible.\n",
      "4) You need to use ChatGPT to extract address, design prompts with attached the webpage text to  ChatGPT, asking it to extract address only, NO explanation or conversation. Use this pre-written function to get response from ChatGPT: helper.get_LLM_reply(prompt=your_prompt_with_webpage_text). Use this statement to extract content from the returned response: response['choices'][0]['message']['content']. \n",
      "5) Save the extracted addresses as \"Address\" column, together with the given 'Provider' and 'Web Site' columns. If there are multile addresses for a provider, each address is a row in the CSV file.\n",
      " \n",
      "Data locations (each data is a node): 1. Autism service provider webpage file location: https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv. The 'Web Site' column is the URL, the 'Provider' column is the provider name. \n",
      "Your reply needs to meet these requirements: \n",
      " 1. Think step by step.\n",
      "2. steps and data (both input and output) form a graph stored in NetworkX. Diconnected components are NOT allowed.\n",
      "3. Each step is a data process operation: the input can be data paths or variables, and the output can be data paths or variables.\n",
      "4. There are two types of nodes: a) operation node, and b) data node (both input and output data). These nodes are also input nodes for the next operation node.\n",
      "5. The input of each operation is the output of the previous operations, except the those need to load data from a path or need to collect data.\n",
      "6. You need to carefully named the output data node.\n",
      "7. The data and operation form a graph.\n",
      "8. The first operations are data loading or collection, and the output of the last operation is the final answer to the task.Operation nodes need to connect via output data nodes, DO NOT connect the operation node directly.\n",
      "9. The node attributes include: 1) node_type (data or operation), 2) data_path (data node only, set to \"\" if not given ), and description. E.g., {‘name’: “County boundary”, “data_type”: “data”, “data_path”: “D:\\Test\\county.shp”,  “description”: “County boundary for the study area”}.\n",
      "10. The connection between a node and an operation node is an edge.\n",
      "11. Add all nodes and edges, including node attributes to a NetworkX instance, DO NOT change the attribute names.\n",
      "12. DO NOT generate code to implement the steps.\n",
      "13. Join the attribute to the vector layer via a common attribute if necessery.\n",
      "14. Put your reply into a Python code block, NO explanation or conversation outside the code block(enclosed by ```python and ```).\n",
      "15. Note that GraphML writer does not support class dict or list as data values.\n",
      "16. You need spatial data (e.g., vector or raster) to make a map.\n",
      "17. Do not put the GraphML writing process as a step in the graph.\n",
      "18. Save the network into GraphML format, save it at: E:\\Research\\LLM-Geo\\Address_extraction\\Address_extraction.graphml \n",
      " \n",
      "Reply example: \n",
      "```python\n",
      "import networkx as nx\n",
      "G = nx.DiGraph()\n",
      "# Add nodes and edges for the graph\n",
      "# 1 Load hazardous waste site shapefile\n",
      "G.add_node(\"haz_waste_shp_url\", node_type=\"data\", path=\"https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/Hazardous_Waste_Sites.zip\", description=\"Hazardous waste facility shapefile URL\")\n",
      "G.add_node(\"load_haz_waste_shp\", node_type=\"operation\", description=\"Load hazardous waste facility shapefile\")\n",
      "G.add_edge(\"haz_waste_shp_url\", \"load_haz_waste_shp\")\n",
      "G.add_node(\"haz_waste_gdf\", node_type=\"data\", description=\"Hazardous waste facility GeoDataFrame\")\n",
      "G.add_edge(\"load_haz_waste_shp\", \"haz_waste_gdf\")\n",
      "...\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 1: population living near hazardous waster\n",
    "'''\n",
    "TASK = r\"\"\"1) Find out the total population that lives within a tract that contain hazardous waste facilities. The study area is North Carolina, US.\n",
    "2) Generate a map to show the spatial distribution of population at the tract level and highlight the borders of tracts that have hazardous waste facilities.\n",
    "\"\"\"\n",
    "\n",
    "DATA_LOCATIONS = [\"NC hazardous waste facility ESRI shape file location: https://github.com/gladcolor/LLM- Geo/raw/master/overlay_analysis/Hazardous_Waste_Sites.zip.\",\n",
    "                  \"NC tract boundary shapefile location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/tract_shp_37.zip. The tract id column is 'Tract'.\",\n",
    "                  \"NC tract population CSV file location: https://github.com/gladcolor/LLM-Geo/raw/master/overlay_analysis/NC_tract_population.csv. The population is stored in 'TotalPopulation' column. The tract ID column is 'GEOID'.\"\n",
    "                 ]\n",
    "'''\n",
    "\n",
    "# Case 2: mobility data retrieval and visulization\n",
    "\"\"\"\n",
    "TASK = r'''\n",
    "1) Show the monthly change rates of each administrative regions in a France map. Each month is a sub-map in a map matrix. The base of the change rate is January 2020. \n",
    "2) Draw a line chart to show the monthly change rate trends of all administrative regeions.\n",
    "\n",
    "'''\n",
    "\n",
    "DATA_LOCATIONS = [\"ESRI shapefile for France administrative regions:\" + \\\n",
    "                  \"https://github.com/gladcolor/LLM-Geo/raw/master/REST_API/France.zip.\" + \\\n",
    "                  \"The 'GID_1' column is the administrative region code, 'NAME_1' column is the administrative region name.\",\n",
    "                  \"REST API url with parameters for mobility data access:\" + \\\n",
    "                  \"http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=world_first_level_admin&begin=01/01/2020&end=12/31/2020.\" + \\\n",
    "                  \"The response is in CSV format. There are three columns in the response: \" + \\\n",
    "                  \"place,date (format:2020-01-07), and intra_movement. 'place' column is the administractive region code, France administrative regions start with 'FRA'.\",\n",
    "                 ]\n",
    "\n",
    "# Bug: \n",
    "# base_month =  \"2020-01\" # wrong: pd.to_datetime(\"2020-01\")\n",
    "# print(region_monthly[region_monthly['month_year'] == base_month])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Case 3: mobility data retrieval and visulization\n",
    "TASK = r\"\"\"1) Find out the Autism service providers' addresses from their website. The address usually listed in the homepage, 'about' or 'contact' page. The latter two pages usually can be found from links in the homepage: theire URLs usually contain 'about' or 'contact'.\n",
    "2) The address shoul contain street, city, state, and zipcode. E.g., 1234 NW Bobcat Lane, St. Robert, MO 65584. A provider may have multiple service address. If you cannot find the address, simply return an empty text, DO NOT make up fake addresses. \n",
    "3) If cannot find the street address, the city, state, or zipcode are also need to return. In summary, find the address information as much as possible.\n",
    "4) You need to use ChatGPT to extract address, design prompts with attached the webpage text to  ChatGPT, asking it to extract address only, NO explanation or conversation. Use this pre-written function to get response from ChatGPT: helper.get_LLM_reply(prompt=your_prompt_with_webpage_text). Use this statement to extract content from the returned response: response['choices'][0]['message']['content']. \n",
    "5) Save the extracted addresses as \"Address\" column, together with the given 'Provider' and 'Web Site' columns. If there are multile addresses for a provider, each address is a row in the CSV file.\n",
    "\"\"\"\n",
    "\n",
    "DATA_LOCATIONS = [\"Autism service provider webpage file location: https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv. The 'Web Site' column is the URL, the 'Provider' column is the provider name.\",                  \n",
    "                 ]\n",
    "\n",
    "\n",
    "# TASK = r'''1)Retrieve the data from the REST API and plot the intra_movement column of the returned data as line chart to show the temporal trend of all states. \n",
    "# 2) plot the temporal trend of the movement for each state. Each state figure will be sub figure in the plot. The plot has 5 columns. In addition, please add a weekly smoothed line to each sub plot, and change the line color to orange.\n",
    "# 3) Using the REST API with date range from 01/01/2020 to 12/31/2020 to analyze the movement reduction rate for each state during two periods: the first period is 01/01/2020-02/29/2020, second period is 03/01/2020 to 04/30/2020. Please find out the reduction rate for each state during the two periods, and create a table to report the result with two columns: state name, reduction rate, sorted by reduction rate.\n",
    "# '''\n",
    "# '''\n",
    "# DATA_LOCATIONS = [\"REST API url with parameters for data access: http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=us_state&begin=01/01/2020&end=12/31/2020; The response is in CSV format. There are three columns in the response: place,date,intra_movement; place refers to the state name.\"\n",
    "#                  ]\n",
    "# '''\n",
    "# 3) Show the administrative region name in the map and chart.\n",
    "# \n",
    "# task_name ='Resident_at_risk_counting'\n",
    "# task_name ='France_mobility_changes_2020'  \n",
    "task_name ='Address_extraction'  \n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), task_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# create graph\n",
    "# model=r\"gpt-3.5-turbo\"\n",
    "model=r\"gpt-4\"\n",
    "model=r\"gpt-4\"\n",
    "solution = Solution(\n",
    "                    task=TASK,\n",
    "                    task_name=task_name,\n",
    "                    save_dir=save_dir,\n",
    "                    data_locations=DATA_LOCATIONS,\n",
    "                    model=model,\n",
    "                    )\n",
    "print(\"Prompt to get solution graph:\\n\")\n",
    "print(solution.graph_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009da7e-afe7-48da-a0c0-62d5a10026fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get graph code from GPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "213b17ca-9e3e-4c23-a852-c4a7edc83448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "\n",
      "Code to generate solution graph: \n",
      "\n",
      "import networkx as nx\n",
      "G = nx.DiGraph()\n",
      "\n",
      "# 1 Load Autism service provider CSV \n",
      "G.add_node(\"provider_csv_url\", node_type=\"data\", path=\"https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv\", description=\"Autism service provider CSV URL\")\n",
      "G.add_node(\"load_provider_csv\", node_type=\"operation\", description=\"Load Autism service provider CSV\")\n",
      "G.add_edge(\"provider_csv_url\", \"load_provider_csv\")\n",
      "\n",
      "G.add_node(\"providers_df\", node_type=\"data\", description=\"Autism service providers DataFrame\")\n",
      "G.add_edge(\"load_provider_csv\", \"providers_df\")\n",
      "\n",
      "# 2 Scrape Autism service provider addresses\n",
      "G.add_node(\"scrape_provider_addresses\", node_type=\"operation\", description=\"Scrape Autism service provider addresses\")\n",
      "G.add_edge(\"providers_df\", \"scrape_provider_addresses\")\n",
      "\n",
      "G.add_node(\"raw_web_page_texts\", node_type=\"data\", description=\"Raw webpage texts from Autism service providers\")\n",
      "G.add_edge(\"scrape_provider_addresses\", \"raw_web_page_texts\")\n",
      "\n",
      "# 3 Extract Addresses using ChatGPT\n",
      "G.add_node(\"extract_addresses_chatgpt\", node_type=\"operation\", description=\"Extract addresses from raw_web_page_texts using ChatGPT\")\n",
      "G.add_edge(\"raw_web_page_texts\", \"extract_addresses_chatgpt\")\n",
      "\n",
      "G.add_node(\"extracted_addresses\", node_type=\"data\", description=\"Extracted addresses from ChatGPT\")\n",
      "G.add_edge(\"extract_addresses_chatgpt\", \"extracted_addresses\")\n",
      "\n",
      "# 4 Merge 'Provider', 'Web Site', and 'Address' columns\n",
      "G.add_node(\"merge_provider_web_site_addresses\", node_type=\"operation\", description=\"Merge 'Provider', 'Web Site', and 'Address' columns\")\n",
      "G.add_edge(\"providers_df\", \"merge_provider_web_site_addresses\")\n",
      "G.add_edge(\"extracted_addresses\", \"merge_provider_web_site_addresses\")\n",
      "\n",
      "G.add_node(\"merged_df\", node_type=\"data\", description=\"Merged DataFrame with 'Provider', 'Web Site', and 'Address' columns\")\n",
      "G.add_edge(\"merge_provider_web_site_addresses\", \"merged_df\")\n",
      "\n",
      "# Save the network to GraphML format\n",
      "nx.write_graphml(G, \"E:\\\\Research\\\\LLM-Geo\\\\Address_extraction\\\\Address_extraction.graphml\")\n"
     ]
    }
   ],
   "source": [
    "response_for_graph = solution.get_LLM_response_for_graph() \n",
    "solution.graph_response = response_for_graph\n",
    "solution.save_solution()\n",
    "print()\n",
    "print(\"Code to generate solution graph: \\n\")\n",
    "print(solution.code_for_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112017eb-8bcb-4d44-88d5-7099f22bf107",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execute code to generate the solution graphto generate the solution graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1695d40-b164-4d8b-8381-957ce260a5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Research\\LLM-Geo\\Address_extraction.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"E:\\Research\\LLM-Geo\\Address_extraction.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x23dce670ee0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(solution.code_for_graph)\n",
    "solution_graph = solution.load_graph_file()\n",
    "\n",
    "# Show the graph\n",
    "G = nx.read_graphml(solution.graph_file)  \n",
    "nt = helper.show_graph(G)\n",
    "html_name = os.path.join(os.getcwd(), solution.task_name + '.html')  \n",
    "# HTML file should in the same directory. See:\n",
    "# https://stackoverflow.com/questions/65564916/error-displaying-pyvis-html-inside-jupyter-lab-cell\n",
    "nt.show(name=html_name)\n",
    "# html_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f8311-1874-4b23-957c-793cfd74a9cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate prompts and code for operations (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "italic-appearance",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with open(r'F:\\Research\\LLM-Geo\\Resident_at_risk_counting\\Resident_at_risk_counting.pkl', 'rb') as f:\n",
    "#     solution = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4459452-8156-476d-8d20-1fccf3fdaa48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 4, load_provider_csv\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "2 / 4, scrape_provider_addresses\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "3 / 4, extract_addresses_chatgpt\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "4 / 4, merge_provider_web_site_addresses\n",
      "Geting LLM reply...\n",
      "Got LLM reply.\n"
     ]
    }
   ],
   "source": [
    "operations = solution.get_LLM_responses_for_operations()\n",
    "solution.save_solution()\n",
    "\n",
    "# all_operation_code_str = '\\n'.join([operation['operation_code'] for operation in operations])\n",
    "# print(\"All operation code: \\n\")\n",
    "# print(all_operation_code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7cc36daf-008e-4159-bc07-23552a9249c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "def load_provider_csv(provider_csv_url=\"https://raw.githubusercontent.com/gladcolor/LLM-Geo/master/Address_extraction/ACE_providers_AGIS.csv\"):\n",
      "    \"\"\"\n",
      "    Description: Load Autism service provider CSV\n",
      "    provider_csv_url: Autism service provider CSV file URL\n",
      "\n",
      "    Returns:\n",
      "    providers_df: A pandas DataFrame containing providers' information from the CSV file\n",
      "    \"\"\"\n",
      "\n",
      "    providers_df = pd.read_csv(provider_csv_url)\n",
      "    return providers_df\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from tqdm import tqdm\n",
      "\n",
      "def scrape_provider_addresses(providers_df):\n",
      "    \"\"\"\n",
      "    Description: Scrape Autism service provider addresses\n",
      "\n",
      "    Input:\n",
      "    providers_df: A pandas DataFrame containing provider information\n",
      "\n",
      "    Output:\n",
      "    raw_web_page_texts: A list of raw web-page texts for each provider\n",
      "    \"\"\"\n",
      "\n",
      "    def get_page_text(url):\n",
      "        try:\n",
      "            response = requests.get(url)\n",
      "            if response.status_code == 200:\n",
      "                return response.text\n",
      "            else:\n",
      "                return None\n",
      "        except Exception as e:\n",
      "            return None\n",
      "            \n",
      "    raw_web_page_texts = {}\n",
      "    \n",
      "    with tqdm(total=len(providers_df)) as progressbar:\n",
      "        for index, row in providers_df.iterrows():\n",
      "            progressbar.update(1)\n",
      "            provider_name, site = row['Provider'], row['Web Site']\n",
      "\n",
      "            about_url = f\"{site}/about\"\n",
      "            contact_url = f\"{site}/contact\"\n",
      "            \n",
      "            homepage_text = get_page_text(site)\n",
      "            about_text = get_page_text(about_url)\n",
      "            contact_text = get_page_text(contact_url)\n",
      "            \n",
      "            raw_texts = {'homepage': homepage_text,\n",
      "                         'about': about_text,\n",
      "                         'contact': contact_text}\n",
      "            \n",
      "            raw_web_page_texts[provider_name] = raw_texts\n",
      "    \n",
      "    return raw_web_page_texts\n",
      "import pandas as pd\n",
      "from tqdm import tqdm\n",
      "from helper import get_LLM_reply\n",
      "\n",
      "def extract_addresses_chatgpt(raw_web_page_texts):\n",
      "    \"\"\"\n",
      "    Description: Extract addresses from raw_web_page_texts using ChatGPT\n",
      "\n",
      "    Input:\n",
      "    raw_web_page_texts: A dictionary of raw web-page texts for each provider\n",
      "\n",
      "    Output:\n",
      "    extracted_addresses: A dictionary containing extracted addresses for each provider\n",
      "    \"\"\"\n",
      "    \n",
      "    def extract_address(provider_name, text_name, raw_text):\n",
      "        if raw_text is None:\n",
      "            return None\n",
      "        else:\n",
      "            prompt = f\"Please extract the address for the following text from {provider_name} {text_name}: {raw_text}\"\n",
      "            response = get_LLM_reply(prompt=prompt)\n",
      "            address = response['choices'][0]['message']['content']\n",
      "        return address\n",
      "\n",
      "    extracted_addresses = {}\n",
      "\n",
      "    with tqdm(total=len(raw_web_page_texts)) as progressbar:\n",
      "        for provider_name, raw_texts in raw_web_page_texts.items():\n",
      "            progressbar.update(1)\n",
      "            extracted_addresses[provider_name] = {}\n",
      "            \n",
      "            for text_name, raw_text in raw_texts.items():\n",
      "                address = extract_address(provider_name, text_name, raw_text)\n",
      "                if address:\n",
      "                    extracted_addresses[provider_name][text_name] = address\n",
      "\n",
      "    return extracted_addresses\n",
      "import pandas as pd\n",
      "\n",
      "def merge_provider_web_site_addresses(providers_df, extracted_addresses):\n",
      "    \"\"\"\n",
      "    Description: Merge 'Provider', 'Web Site', and 'Address' columns\n",
      "\n",
      "    Input:\n",
      "    providers_df: A pandas DataFrame containing provider information\n",
      "    extracted_addresses: A dictionary containing extracted addresses for each provider\n",
      "\n",
      "    Output:\n",
      "    merged_df: A pandas DataFrame containing 'Provider', 'Web Site', and 'Address' columns merged\n",
      "    \"\"\"\n",
      "\n",
      "    rows = []\n",
      "\n",
      "    for provider_name, web_site_addresses in extracted_addresses.items():\n",
      "        for text_name, address in web_site_addresses.items():\n",
      "            row = {'Provider': provider_name,\n",
      "                   'Web Site': providers_df.loc[providers_df['Provider'] == provider_name, 'Web Site'].values[0],\n",
      "                   'Address': address}\n",
      "            rows.append(row)\n",
      "\n",
      "    merged_df = pd.DataFrame(rows)\n",
      "    return merged_df\n"
     ]
    }
   ],
   "source": [
    "all_operation_code_str = '\\n'.join([operation['operation_code'] for operation in operations])\n",
    "exec(all_operation_code_str)\n",
    "print(all_operation_code_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21f735-29f4-4934-8a7e-00616447cd40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate prompts and code for assembly program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34ac055d-11b8-4b3e-890c-83501611355f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geting LLM reply...\n",
      "Got LLM reply.\n",
      "# Load the Autism service provider CSV\n",
      "providers_df = load_provider_csv()\n",
      "\n",
      "# Scrape addresses from their webpages\n",
      "raw_texts = scrape_provider_addresses(providers_df)\n",
      "\n",
      "# Extract addresses using ChatGPT\n",
      "extracted_addresses = extract_addresses_chatgpt(raw_texts)\n",
      "\n",
      "# Merge 'Provider', 'Web Site', and 'Address' columns\n",
      "merged_addresses_df = merge_provider_web_site_addresses(providers_df, extracted_addresses)\n",
      "\n",
      "# Save the merged dataframe to a CSV file\n",
      "merged_addresses_df.to_csv(\"ACE_providers_with_addresses.csv\", index=False)\n",
      "Assembly code: \n",
      "\n",
      "# Load the Autism service provider CSV\n",
      "providers_df = load_provider_csv()\n",
      "\n",
      "# Scrape addresses from their webpages\n",
      "raw_texts = scrape_provider_addresses(providers_df)\n",
      "\n",
      "# Extract addresses using ChatGPT\n",
      "extracted_addresses = extract_addresses_chatgpt(raw_texts)\n",
      "\n",
      "# Merge 'Provider', 'Web Site', and 'Address' columns\n",
      "merged_addresses_df = merge_provider_web_site_addresses(providers_df, extracted_addresses)\n",
      "\n",
      "# Save the merged dataframe to a CSV file\n",
      "merged_addresses_df.to_csv(\"ACE_providers_with_addresses.csv\", index=False)\n"
     ]
    }
   ],
   "source": [
    "assembly_LLM_response = solution.get_LLM_assembly_response()\n",
    "# solution.assembly_LLM_response = assembly_LLM_response\n",
    "solution.save_solution()\n",
    "\n",
    "print(\"Assembly code: \\n\")\n",
    "print(solution.code_for_assembly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca67352-6508-4fe6-be4b-1f4649224148",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execute assembly code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "583d7c6a-8d5a-4bca-b22e-fcb3f3d5c201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load the Autism service provider CSV\n",
      "providers_df = load_provider_csv()\n",
      "\n",
      "# Scrape addresses from their webpages\n",
      "raw_texts = scrape_provider_addresses(providers_df)\n",
      "\n",
      "# Extract addresses using ChatGPT\n",
      "extracted_addresses = extract_addresses_chatgpt(raw_texts)\n",
      "\n",
      "# Merge 'Provider', 'Web Site', and 'Address' columns\n",
      "merged_addresses_df = merge_provider_web_site_addresses(providers_df, extracted_addresses)\n",
      "\n",
      "# Save the merged dataframe to a CSV file\n",
      "merged_addresses_df.to_csv(\"ACE_providers_with_addresses.csv\", index=False)\n"
     ]
    }
   ],
   "source": [
    "all_code = all_operation_code_str + '\\n' + solution.code_for_assembly\n",
    "print(solution.code_for_assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90365390-4b0a-4164-8b25-5179d13da5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [02:16<00:00,  2.73s/it]\n",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geting LLM reply...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                 | 1/50 [00:00<00:42,  1.15it/s]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 88987 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:129\u001b[0m\n",
      "File \u001b[1;32m<string>:92\u001b[0m, in \u001b[0;36mextract_addresses_chatgpt\u001b[1;34m(raw_web_page_texts)\u001b[0m\n",
      "File \u001b[1;32m<string>:80\u001b[0m, in \u001b[0;36mextract_address\u001b[1;34m(provider_name, text_name, raw_text)\u001b[0m\n",
      "File \u001b[1;32mE:\\Research\\LLM-Geo\\helper.py:48\u001b[0m, in \u001b[0;36mget_LLM_reply\u001b[1;34m(prompt, system_role, model, verbose)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeting LLM reply...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_role\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot LLM reply.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\street_mapping_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\street_mapping_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\street_mapping_env\\lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\street_mapping_env\\lib\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\street_mapping_env\\lib\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 88987 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "exec(all_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c61f4-fa4d-4456-8fc5-1759c6198ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5aac09-fb66-4d6a-978d-3be5c25d3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Case 2: mobility \n",
    "# import geopandas as gpd\n",
    "\n",
    "# def load_france_shp(france_shp_url=\"https://github.com/gladcolor/LLM-Geo/raw/master/REST_API/France.zip\"):\n",
    "#     \"\"\"\n",
    "#     Description: Load France administrative regions shapefile\n",
    "\n",
    "#     Args:\n",
    "#     france_shp_url (str): URL of the zipped shapefile\n",
    "\n",
    "#     Returns:\n",
    "#     france_gdf (GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "#     \"\"\"\n",
    "#     france_gdf = gpd.read_file(france_shp_url)\n",
    "#     return france_gdf\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import requests\n",
    "# from io import StringIO\n",
    "\n",
    "# def load_mobility_data(mobility_api_url=\"http://gis.cas.sc.edu/GeoAnalytics/REST?operation=get_daily_movement_for_all_places&source=twitter&scale=world_first_level_admin&begin=01/01/2020&end=12/31/2020\"):\n",
    "#     \"\"\"\n",
    "#     Load COVID-19 mobility data\n",
    "\n",
    "#     mobility_api_url: REST API url for COVID-19 mobility data\n",
    "#     returns: mobility_data (dataframe): Mobility data with place, date and intra_movement values\n",
    "#     \"\"\"\n",
    "#     response = requests.get(mobility_api_url)\n",
    "#     data = StringIO(response.text)\n",
    "#     mobility_data = pd.read_csv(data)\n",
    "\n",
    "#     return mobility_data\n",
    "# def join_data(france_gdf=gpd.GeoDataFrame(), mobility_data=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Join shapefile and mobility data\n",
    "\n",
    "#     Args:\n",
    "#     france_gdf (GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "#     mobility_data (pd.DataFrame): Mobility data with place, date, and intra_movement values\n",
    "\n",
    "#     Returns:\n",
    "#     joined_data (GeoDataFrame): GeoDataFrame containing the joined shapefile and mobility data\n",
    "#     \"\"\"\n",
    "#     # Convert GID_1 column to string type without leading zeros\n",
    "#     france_gdf[\"GID_1\"] = france_gdf[\"GID_1\"].astype(str).str.lstrip(\"0\")\n",
    "\n",
    "#     # Convert place column to string type without leading zeros\n",
    "#     mobility_data[\"place\"] = mobility_data[\"place\"].astype(str).str.lstrip(\"0\")\n",
    "\n",
    "#     # Merge mobility data with shapefile based on GID_1 and place columns\n",
    "#     joined_data = france_gdf.merge(mobility_data, left_on=\"GID_1\", right_on=\"place\")\n",
    "\n",
    "#     # Remove duplicates\n",
    "#     joined_data.drop_duplicates(subset=[\"GID_1\", \"date\"], inplace=True)\n",
    "\n",
    "#     return joined_data\n",
    "# def calc_monthly_changes(joined_data=gpd.GeoDataFrame()):\n",
    "#     \"\"\"\n",
    "#     Create monthly change rates\n",
    "    \n",
    "#     Args:\n",
    "#     joined_data (gpd.GeoDataFrame): GeoDataFrame containing the joined shapefile and mobility data\n",
    "    \n",
    "#     Returns:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "#     \"\"\"\n",
    "#     # Parse dates and create a month-year column\n",
    "#     joined_data['date'] = pd.to_datetime(joined_data['date'])\n",
    "#     joined_data['month_year'] = joined_data['date'].dt.to_period('M')\n",
    "\n",
    "#     # Calculate the sum of intra_movement for each region and month-year\n",
    "#     region_monthly = joined_data.groupby(['NAME_1', 'month_year'])['intra_movement'].sum().reset_index()\n",
    "\n",
    "#     # Calculate the changes of each month based on January 2020\n",
    "#     base_month =  \"2020-01\" # pd.to_datetime(\"2020-01\")\n",
    "#     print(region_monthly[region_monthly['month_year'] == \"2020-01\"])\n",
    "#     baseline = region_monthly[region_monthly['month_year'] == base_month].set_index('NAME_1')['intra_movement']\n",
    "#     print(baseline)\n",
    "#     region_monthly['change_rate'] = region_monthly.apply(lambda row: (row['intra_movement'] - baseline[row['NAME_1']]) / baseline[row['NAME_1']], axis=1)\n",
    "\n",
    "#     # Pivot month_year as columns to create monthly_changes DataFrame\n",
    "#     monthly_changes = region_monthly.pivot(index='NAME_1', columns='month_year', values='change_rate').reset_index()\n",
    "\n",
    "#     return monthly_changes\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def matrix_map(monthly_changes=pd.DataFrame(), france_gdf=gpd.GeoDataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Create matrix of France maps\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "#     france_gdf (gpd.GeoDataFrame): GeoDataFrame containing the France administrative regions\n",
    "\n",
    "#     Returns:\n",
    "#     france_map_fig (Figure): Figure containing the matrix of France maps with monthly change rates\n",
    "#     \"\"\"\n",
    "#     # Merge monthly_changes with france_gdf\n",
    "#     france_gdf = france_gdf.merge(monthly_changes, on=\"NAME_1\")\n",
    "    \n",
    "#     # Set up plot\n",
    "#     plt.style.use('seaborn-darkgrid')\n",
    "#     france_map_fig, axs = plt.subplots(3, 4, figsize=(24, 18), sharex='col', sharey='row')\n",
    "#     france_map_fig.suptitle('Monthly change rates of each administrative region in France')\n",
    "\n",
    "#     # Iterate through columns for each month\n",
    "#     for idx, month_year in enumerate(monthly_changes.columns[1:]):\n",
    "#         row, col = divmod(idx, 4)\n",
    "        \n",
    "#         # Create subplot\n",
    "#         france_gdf.plot(column=month_year, ax=axs[row, col], legend=True, cmap='coolwarm', legend_kwds={'shrink': 0.8})\n",
    "#         axs[row, col].set_title(month_year.strftime(\"%B %Y\"))\n",
    "#         axs[row, col].set_xticks([])\n",
    "#         axs[row, col].set_yticks([])\n",
    "#         axs[row, col].set_xlabel(\"\")\n",
    "#         axs[row, col].set_ylabel(\"\")\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "#     return france_map_fig\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def chart_matrix(monthly_changes=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Create chart matrix of each administrative region's line chart\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "\n",
    "#     Returns:\n",
    "#     line_chart_matrix_fig (matplotlib.figure.Figure): Figure object containing the line chart matrix\n",
    "#     \"\"\"\n",
    "#     # Set default seaborn style\n",
    "#     sns.set()\n",
    "\n",
    "#     # Prepare monthly_changes data\n",
    "#     monthly_changes_tidy = pd.melt(monthly_changes, \n",
    "#                                    id_vars=[\"NAME_1\"], \n",
    "#                                    var_name=\"month_year\", \n",
    "#                                    value_name=\"change_rate\")\n",
    "#     monthly_changes_tidy[\"month_year\"] = monthly_changes_tidy[\"month_year\"].astype(str)\n",
    "\n",
    "#     # Calculate the number of rows and columns for the line chart matrix\n",
    "#     n_regions = len(monthly_changes[\"NAME_1\"].unique())\n",
    "#     n_cols = min(4, n_regions)\n",
    "#     n_rows = (n_regions // n_cols) + (1 if (n_regions % n_cols) else 0)\n",
    "\n",
    "#     # Create the line chart matrix\n",
    "#     line_chart_matrix_fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5), sharex=True, sharey=True)\n",
    "\n",
    "#     for idx, (name, group) in enumerate(monthly_changes_tidy.groupby([\"NAME_1\"])):\n",
    "#         row, col = divmod(idx, n_cols)\n",
    "\n",
    "#         ax = axes[row][col]\n",
    "#         sns.lineplot(x=\"month_year\", y=\"change_rate\", data=group, ax=ax)\n",
    "#         ax.set_title(name)\n",
    "#         ax.set_xticklabels(labels=group[\"month_year\"].unique(), rotation=45)\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for idx in range(n_regions, n_rows * n_cols):\n",
    "#         row, col = divmod(idx, n_cols)\n",
    "#         axes[row][col].remove()\n",
    "\n",
    "#     # Adjust layout\n",
    "#     line_chart_matrix_fig.tight_layout()\n",
    "\n",
    "#     return line_chart_matrix_fig\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def overall_trends_line_chart(monthly_changes=pd.DataFrame()):\n",
    "#     \"\"\"\n",
    "#     Description: Draw the line chart for all administrative regions\n",
    "\n",
    "#     Args:\n",
    "#     monthly_changes (pd.DataFrame): DataFrame with monthly change rates for each administrative region\n",
    "\n",
    "#     Returns:\n",
    "#     trends_line_chart_fig (matplotlib.figure.Figure): Figure containing the line chart for all administrative regions\n",
    "#     \"\"\"\n",
    "#     # Transpose monthly_changes DataFrame\n",
    "#     transposed_changes = monthly_changes.set_index(\"NAME_1\").transpose()\n",
    "\n",
    "#     # Create line chart\n",
    "#     trends_line_chart_fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     transposed_changes.plot(ax=ax)\n",
    "#     plt.xlabel(\"Month\")\n",
    "#     plt.ylabel(\"Change Rate\")\n",
    "#     plt.title(\"Monthly Trends of Change Rates for All Administrative Regions\")\n",
    "#     plt.legend(title=\"Regions\", labels=transposed_changes.columns, bbox_to_anchor=(1.17, 1))\n",
    "#     plt.xticks(range(len(transposed_changes.index)), transposed_changes.index, rotation=90)\n",
    "\n",
    "#     return trends_line_chart_fig\n",
    "\n",
    "# # Example usage\n",
    "# # france_gdf = load_france_shp()\n",
    "# # mobility_data = load_mobility_data()\n",
    "# # joined_data = join_data(france_gdf, mobility_data)\n",
    "# # monthly_changes = calc_monthly_changes(joined_data)\n",
    "# # trends_line_chart_fig = overall_trends_line_chart(monthly_changes)\n",
    "# # plt.show()\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from io import StringIO\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    " \n",
    "# # Load data\n",
    "# monthly_changes = calc_monthly_changes(joined_data)\n",
    "\n",
    "# # Create and save monthly change maps\n",
    "# france_map_fig = matrix_map(monthly_changes, france_gdf)\n",
    "# france_map_fig.savefig('france_monthly_change_maps.png')\n",
    "\n",
    "# # Create and save chart matrix with line charts of each administrative region\n",
    "# line_chart_matrix_fig = chart_matrix(monthly_changes)\n",
    "# line_chart_matrix_fig.savefig('chart_matrix_line_charts.png')\n",
    "\n",
    "# # Create and save overall trends line chart\n",
    "# trends_line_chart_fig = overall_trends_line_chart(monthly_changes)\n",
    "# trends_line_chart_fig.savefig('overall_trends_line_chart.png')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111a633-11ad-4501-83b0-658a81353c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data#.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "street_mapping_env",
   "language": "python",
   "name": "street_mapping_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
